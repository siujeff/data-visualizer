{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Data Visualizer ‚Äî Workflow Guide\n",
        "\n",
        "**Data Visualizer** follows the **Extract ‚Üí Transform ‚Üí Load (ETL)** flow to ingest **structured datasets** from public resources. It helps you **preview, clean, and visualize** data power by LLM, then (optionally) **import** it into a database (e.g., Snowflake, Supabase, PostgreSQL).  \n",
        "\n",
        "## **Overview**\n",
        "\n",
        "- **Data source:** UCI Machine Learning Repository, Data.gov, or your own CSV/ZIP uploads  \n",
        "- **Tasks:** Preview data ‚Üí Detect anomalies ‚Üí Generate charts ‚Üí AI summary  \n",
        "- **Destinations (WIP):** Snowflake, Supabase, PostgreSQL\n",
        "- **Data privacy:** Data Visualizer runs in your own Google runtime. If you prefer not to use Colab, download the notebook and run it locally.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Quick Start\n",
        " **How to use this notebook**\n",
        "1. Work **top-to-bottom**.\n",
        "2. In each section labeled **Step #**, click **‚ñ∂Ô∏è Run**.\n",
        "3. If a cell has a form, fill it out **before** pressing ‚ñ∂Ô∏è Run.\n",
        "4. Review outputs in the **Results/Preview** panel beneath each step.\n",
        "\n",
        "> **Tip:** If output keeps stacking, re-run the step‚Äîold output panels are cleared automatically.\n",
        "\n",
        "---\n",
        "<details>\n",
        "  <summary>More information</summary>\n",
        "\n",
        "## Part 1 ‚Äî Pick a Dataset\n",
        "\n",
        "Choose **one** of the following and press **‚ñ∂Ô∏è Run**:\n",
        "- **UCI (Search):** Type a keyword (e.g., *iris*, *heart*, *wine*) ‚Üí **Search** ‚Üí **Load Selected**  \n",
        "- **Data.gov (Search):** Type a keyword (e.g., *medicare*, *CMS*) ‚Üí **Search** ‚Üí **Load Selected**  \n",
        "- **Upload CSV/ZIP:** Click **Upload**, select files, then **Run Loader**  \n",
        "- **Paste URL (CSV):** Paste a direct CSV link and **Run Loader**\n",
        "\n",
        "**Output:** A `DATASETS` dictionary containing one or more pandas DataFrames.\n",
        "\n",
        "\n",
        "## Part 2 ‚Äî Explore & Analyze\n",
        "\n",
        "Click **‚ñ∂Ô∏è Run** on each analysis cell to:\n",
        "\n",
        "- **Preview** the first rows and **summary statistics**  \n",
        "- **Check data quality:** missing values, type mismatches, outliers  \n",
        "- **Suggest charts:** quick ideas for histograms, scatter, and category plots  \n",
        "- **AI summary (optional):** If you provide a Gemini API key, get a plain-English overview\n",
        "\n",
        "> **Tip:** If an analysis cell mentions ‚ÄúLoad a dataset first,‚Äù go back to **Part 1** and load data again.\n",
        "\n",
        "\n",
        "## Part 3 ‚Äî Load (WIP)\n",
        "\n",
        "Select a target and press **‚ñ∂Ô∏è Run**:\n",
        "- **Destinations:** Snowflake, Supabase, PostgreSQL  \n",
        "- **Credentials:** Provide URL, API key/token, or connection string  \n",
        "- **Schema:** Tool can infer basic types; review before confirming\n",
        "\n",
        "> **Security:** API keys are entered in a **password widget** and are **not stored** in your notebook file.\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ Notes\n",
        "\n",
        "- The AI summary will only run if **Gemini** is configured in **Step 2**.  \n",
        "- Some Data.gov resources block direct CSV fetch; the loader will attempt alternate methods automatically.\n",
        "- A dict like: `{'uci_123_iris': DataFrame, 'datagov_hospital_ratings': DataFrame, ...}`\n",
        "- Access with `DATASETS[\"name\"]` to use a specific DataFrame in custom code.\n",
        "<br><br>\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Troubleshooting\n",
        "\n",
        "- **403 when downloading CSV from Data.gov:**  \n",
        "  The loader retries with browser-style headers and CKAN API fallbacks.\n",
        "- **No CSVs found:**  \n",
        "  Check your folder name and that files end in `.csv` or `.data`.\n",
        "- **Large files:**  \n",
        "  Preview is limited (head + describe) to keep the notebook responsive, if colab ran out of RAM due to the large amount of data. Download the notebook and run it in your own local enviorment.\n",
        "- **Additional data doesn't belong to the set:**  \n",
        "  Delete any previously loaded data from the coblab upload folder (default foldername: **datasets**) if you see additional data loaded.\n",
        "\n",
        "## üìß Need help?\n",
        "\n",
        "- Contact: siujeff [at] outlook [dot] com for questions and feedback.\n",
        "\n",
        "---\n",
        "</details>"
      ],
      "metadata": {
        "id": "Zc4POA6iR5Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 1 ‚Äî Install libraries and functions** { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click the **‚ñ∂Ô∏è Run** button on the left of this cell.\n",
        "#@markdown <br>**What this does:** Installs abd initialize required packages, libraries, and functions to run this notebook.\n",
        "#@markdown _**Tips:** Click ‚ÄúShow code‚Äù if you want to see the details._\n",
        "\n",
        "%%capture\n",
        "!pip install -q ucimlrepo\n",
        "!pip install -q -U supabase\n",
        "\n",
        "from supabase import create_client, Client\n",
        "import google.generativeai as genai\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output, Markdown\n",
        "from IPython.utils import io\n",
        "from google.colab import files, output\n",
        "import io, struct, csv, re, os, subprocess, shutil, json, requests, time, traceback, threading, socket, datetime, zipfile, csv, pathlib, math\n",
        "import base64, zlib\n",
        "import google.generativeai as genai\n",
        "import pandas as pd, numpy as np, requests, re, os, json, socket, datetime, csv\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Any, Tuple, Optional, Union\n",
        "from urllib.parse import urlparse as _urlparse, quote\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "# --- Functions ---\n",
        "def clean_chunk(df):\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df = df.where(pd.notnull(df), None)\n",
        "    return df.astype(object)\n",
        "\n",
        "def is_json_serializable(record):\n",
        "    try:\n",
        "        json.dumps(record, allow_nan=False); return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def convert_nan_to_none(d):\n",
        "    return {k: (None if isinstance(v, float) and np.isnan(v) else v) for k, v in d.items()}\n",
        "\n",
        "def insert_csv_to_supabase_in_chunks(table_name: str, supabase_url: str, supabase_key: str):\n",
        "    \"\"\"Reads {table_name}.csv and POSTs in chunks via REST.\"\"\"\n",
        "    csv_path = f\"{table_name}.csv\"\n",
        "    api_url = f\"{supabase_url}/rest/v1/{table_name}\"\n",
        "    headers = {\n",
        "        \"apikey\": supabase_key,\n",
        "        \"Authorization\": f\"Bearer {supabase_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Prefer\": \"resolution=merge-duplicates\"\n",
        "    }\n",
        "    for chunk in pd.read_csv(csv_path, chunksize=1000):\n",
        "        chunk = clean_chunk(chunk)\n",
        "        payload = [convert_nan_to_none(r) for r in chunk.to_dict(orient=\"records\")]\n",
        "        payload = [r for r in payload if is_json_serializable(r)]\n",
        "        if not payload:\n",
        "            print(\"‚ö†Ô∏è Entire chunk was non-serializable, skipping.\"); continue\n",
        "        r = requests.post(api_url, json=payload, headers=headers)\n",
        "        r.raise_for_status()\n",
        "        print(f\"‚úÖ Uploaded {len(payload)} records\")\n",
        "\n",
        "def get_column_name_from_csv():\n",
        "    column_info, column_datatype_info = {}, {}\n",
        "    csv_files = [f for f in os.listdir() if f.endswith(\".csv\")]\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            column_info[csv_file[:-4]] = df.columns.tolist()\n",
        "            column_datatype_info[csv_file] = df.dtypes.apply(lambda x: x.name).to_dict()\n",
        "        except Exception as e:\n",
        "            column_info[csv_file] = f\"Error reading file: {e}\"\n",
        "    table_names = [csv_file[:-4] for csv_file in csv_files]\n",
        "    return table_names, column_info, column_datatype_info\n",
        "\n",
        "def create_dataframes_from_csv(folder_path=\".\"):\n",
        "    dataframes = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(os.path.join(folder_path, filename))\n",
        "                dataframes[filename[:-4]] = df\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {e}\")\n",
        "    return dataframes\n",
        "\n",
        "def test_database_connection(supabase_url: str, supabase_key: str):\n",
        "    try:\n",
        "        socket.create_connection((\"google.com\", 80), timeout=5); print(\"Network connectivity: OK\")\n",
        "    except OSError as e:\n",
        "        print(f\"Network connectivity: FAIL - {e}\")\n",
        "    try:\n",
        "        host = supabase_url.replace(\"https://\", \"\").split(\"/\")[0]\n",
        "        socket.gethostbyname(host); print(\"DNS resolution: OK\")\n",
        "    except socket.gaierror as e:\n",
        "        print(f\"DNS resolution: FAIL - {e}\")\n",
        "    try:\n",
        "        sb: Client = create_client(supabase_url, supabase_key)\n",
        "        r = requests.get(f\"{supabase_url}/rest/v1\", timeout=8)\n",
        "        print(\"Supabase project reachable:\", r.status_code)\n",
        "        return sb\n",
        "    except Exception as e:\n",
        "        print(\"Supabase not reachable:\", e); return None\n",
        "\n",
        "def create_supabase_table(sb: Client, table_name, df):\n",
        "    \"\"\"Print CREATE TABLE SQL (execute this in Supabase SQL editor).\"\"\"\n",
        "    cols = []\n",
        "    for col_name, col_type in df.dtypes.items():\n",
        "        if pd.api.types.is_integer_dtype(col_type):  sql_type = \"INT\"\n",
        "        elif pd.api.types.is_float_dtype(col_type): sql_type = \"FLOAT8\"\n",
        "        elif pd.api.types.is_datetime64_any_dtype(col_type): sql_type = \"TIMESTAMPTZ\"\n",
        "        else: sql_type = \"TEXT\"\n",
        "        cols.append(f'\"{col_name}\" {sql_type}')\n",
        "    create_table_sql = f'CREATE TABLE IF NOT EXISTS \"{table_name}\" ({\", \".join(cols)});'\n",
        "    print(f\"Run this in SQL editor:\\n{create_table_sql}\")\n",
        "\n",
        "def is_valid_sql_name(name):\n",
        "    return bool(name) and len(name) <= 63 and re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', name)\n",
        "\n",
        "def fix_invalid_name(name):\n",
        "    original = name\n",
        "    name = re.sub(r'[^a-zA-Z0-9_]', '_', name)\n",
        "    if not re.match(r'^[a-zA-Z_]', name): name = '_' + name\n",
        "    if len(name) > 63: name = name[:63]\n",
        "    if name != original: print(f\"Invalid table name '{original}' fixed to '{name}'.\")\n",
        "    return name\n",
        "\n",
        "def data_log(log_message):\n",
        "    with open(\"etl_transformation.log\", \"a\") as f:\n",
        "        f.write(f\"{datetime.datetime.now()}: {log_message}\\n\")\n",
        "\n",
        "def extract_uci_dataset_id(url: str):\n",
        "    try:\n",
        "        path = urlparse(url).path\n",
        "    except Exception:\n",
        "        path = str(url)\n",
        "    m = re.search(r'/dataset/(\\d+)(?:/|$)', path)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def extract_ids_from_input(urls_csv: str):\n",
        "    ids = []\n",
        "    for u in [s.strip() for s in urls_csv.split(\",\") if s.strip()]:\n",
        "        dsid = extract_uci_dataset_id(u)\n",
        "        if dsid is not None: ids.append(dsid)\n",
        "    return ids\n"
      ],
      "metadata": {
        "id": "wNXF8Kx5aUM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 2 ‚Äî Optional: Enable Gemini features** (Get your API key at https://aistudio.google.com/app/apikey) { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click **‚ñ∂Ô∏è Run**.\n",
        "#@markdown <br>**What this does:** Lets you use Gemini for AI features. Your key is protected and will not be saved in the notebook.\n",
        "\n",
        "# --- Widgets ---\n",
        "gemini_api_key_input = widgets.Password(\n",
        "    description='Gemini API Key:',\n",
        "    value='',\n",
        "    layout=widgets.Layout(width='700px'),\n",
        "    style={'description_width': '95px'}\n",
        ")\n",
        "\n",
        "gemini_model_selector = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Gemini-2.5-pro (Solid reasoning)',          'gemini-2.5-pro'),\n",
        "        ('Gemini-2.5-flash (Best price/performance)', 'gemini-2.5-flash'),\n",
        "        ('Gemini-2.5-flash-lite (Fast & cheap)',      'gemini-2.5-flash-lite'),\n",
        "        ('Gemini-2.0-flash (Good baseline)',          'gemini-2.0-flash'),\n",
        "    ],\n",
        "    value='gemini-2.5-flash',\n",
        "    description='Gemini Model:',\n",
        "    style={'description_width': '95px'},\n",
        "    layout=widgets.Layout(width='700px')\n",
        ")\n",
        "\n",
        "btn_save_gemini = widgets.Button(description='Save & Test Gemini', button_style='primary')\n",
        "out_gemini = widgets.Output()\n",
        "\n",
        "# --- State (no key stored here) ---\n",
        "_gemini_model = None\n",
        "\n",
        "def get_selected_model_name():\n",
        "    return gemini_model_selector.value\n",
        "\n",
        "def save_and_test_gemini(_):\n",
        "    global _gemini_model\n",
        "    with out_gemini:\n",
        "        clear_output()\n",
        "        key = (gemini_api_key_input.value or '').strip()\n",
        "        model_name = get_selected_model_name()\n",
        "        if not key:\n",
        "            print(\"‚ö†Ô∏è Please enter your Gemini API key.\")\n",
        "            return\n",
        "        try:\n",
        "            # Configure with the provided key (not stored globally)\n",
        "            genai.configure(api_key=key)\n",
        "            test_model = genai.GenerativeModel(model_name)\n",
        "            resp = test_model.generate_content(\"Reply with the word: READY\")\n",
        "            if \"READY\" in (resp.text or \"\").upper():\n",
        "                _gemini_model = test_model\n",
        "                # Keep the key only in the environment, then clear references\n",
        "                os.environ['GEMINI_API_KEY'] = key\n",
        "                gemini_api_key_input.value = ''   # clears the password box\n",
        "                del key                           # drop local reference\n",
        "                print(f\"‚úÖ Gemini connected. Model: {model_name} ready to use.\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Connected, but unexpected response. Try another model.\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Gemini test failed:\", e)\n",
        "\n",
        "btn_save_gemini.on_click(save_and_test_gemini)\n",
        "\n",
        "def gemini_chat(prompt: str, system: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Calls Gemini using the in-memory model initialized above.\n",
        "    Does not read or expose your API key.\n",
        "    \"\"\"\n",
        "    if _gemini_model is None:\n",
        "        raise RuntimeError(\"Gemini not initialized. Click 'Save & Test Gemini' first.\")\n",
        "    content = prompt if system is None else [{\"role\":\"user\",\"parts\":[prompt]}]\n",
        "    resp = _gemini_model.generate_content(content)\n",
        "    return (resp.text or \"\").strip()\n",
        "\n",
        "display(\n",
        "    widgets.VBox([\n",
        "        widgets.HTML(\"<b>Gemini Setup</b>\"),\n",
        "        gemini_api_key_input,\n",
        "        gemini_model_selector,\n",
        "        btn_save_gemini,\n",
        "        out_gemini\n",
        "    ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z-uo8iDjYqGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 3 ‚Äî Select a data source you would like to load (UCI / Data.gov / CSV)** { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click **‚ñ∂Ô∏è Run**.\n",
        "#@markdown <br>**What this does:** Allow you to choose a dataset from a public data source.\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "except Exception:\n",
        "    files = None\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "# --- UI: explicit modes (added data.gov search) ---\n",
        "data_source_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('UCI Data Repository (Search)', 'uci_search'),\n",
        "        ('Data.gov Public Dataset (Search)', 'datagov_search'),\n",
        "        ('Dataset URL (csv)', 'uci_paste'),\n",
        "        ('My own data (upload CSV/ZIP)', 'csv')\n",
        "    ],\n",
        "    value='uci_search',\n",
        "    description='Data source:',\n",
        "    layout=widgets.Layout(width='420px')\n",
        ")\n",
        "\n",
        "# common settings\n",
        "target_folder_text = widgets.Text(value='datasets', description='Save to:', layout=widgets.Layout(width='300px'))\n",
        "preview_rows_int   = widgets.BoundedIntText(value=5, min=1, max=50, step=1, description='Preview rows:')\n",
        "\n",
        "save_csv_check     = widgets.Checkbox(value=True, description='Also save CSV to üìÅ')\n",
        "settings_row       = widgets.HBox([target_folder_text, preview_rows_int, save_csv_check])\n",
        "\n",
        "# ---------------- UCI search section ----------------\n",
        "uci_search_text = widgets.Text(\n",
        "    value='', placeholder='Search UCI data repository (e.g., medical, patient, diabetes)',\n",
        "    description='Keywords:', layout=widgets.Layout(width='500px'),\n",
        "    style={'description_width': '80px'}\n",
        ")\n",
        "btn_search_uci = widgets.Button(description='Search UCI')\n",
        "uci_results_dropdown = widgets.Dropdown(options=[], description='Results:', layout=widgets.Layout(width='600px'),\n",
        "                                        style={'description_width': '80px'})\n",
        "btn_load_selected_uci = widgets.Button(description='Load Selected', button_style='primary')\n",
        "uci_pick_section = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Enter anything in search below:</b>\"),\n",
        "    widgets.HBox([uci_search_text, btn_search_uci]),\n",
        "    widgets.HBox([uci_results_dropdown, btn_load_selected_uci]),\n",
        "])\n",
        "\n",
        "# ---------------- data.gov search section ----------------\n",
        "datagov_search_text = widgets.Text(\n",
        "    value='', placeholder='Search dataset from Data.gov (e.g., medicare, HEDIS, CMS Star)',\n",
        "    description='KeywordSearch:', layout=widgets.Layout(width='500px'),\n",
        "    style={'description_width': '80px'}\n",
        ")\n",
        "btn_search_datagov = widgets.Button(description='Search data.gov')\n",
        "datagov_results_dropdown = widgets.Dropdown(options=[], description='Results:', layout=widgets.Layout(width='600px'),\n",
        "                                            style={'description_width': '80px'})\n",
        "btn_load_selected_datagov = widgets.Button(description='Load Selected', button_style='primary')\n",
        "datagov_pick_section = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Pick from data.gov (CSV resources)</b>\"),\n",
        "    widgets.HBox([datagov_search_text, btn_search_datagov]),\n",
        "    widgets.HBox([datagov_results_dropdown, btn_load_selected_datagov]),\n",
        "])\n",
        "\n",
        "# ---------------- Paste section (IDs/URLs) ----------------\n",
        "uci_urls_text = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='(Experimental) Require Gemini API for non-CSV dataset extraction',\n",
        "    description='IDs/URLs:',\n",
        "    layout=widgets.Layout(width='700px')\n",
        ")\n",
        "uci_paste_section = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Extract dataset from URL</b>\"),\n",
        "    uci_urls_text,\n",
        "    widgets.HTML('<span style=\"font-size:12px;color:#666\">Examples: https://example.com/file.csv</span>')\n",
        "])\n",
        "\n",
        "# ---------------- Action row ----------------\n",
        "upload_button = widgets.Button(description='Upload CSV/ZIP')\n",
        "run_button    = widgets.Button(description='Run Loader', button_style='primary')\n",
        "action_row    = widgets.HBox([upload_button, run_button])\n",
        "_BROWSER_HEADERS = {\n",
        "    # A modern browser UA\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "}\n",
        "\n",
        "_HTTP_HEADERS = {\n",
        "    \"User-Agent\": _BROWSER_HEADERS[\"User-Agent\"],\n",
        "    \"Accept\": \"text/csv,application/octet-stream;q=0.9,*/*;q=0.8\",\n",
        "}\n",
        "\n",
        "# --- Helpers ---\n",
        "def _is_http_url(s: str) -> bool:\n",
        "    try:\n",
        "        return _urlparse(str(s)).scheme in (\"http\", \"https\")\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _download_browserish_csv(url: str) -> bytes:\n",
        "    \"\"\"\n",
        "    For CKAN-style /dataset/.../resource/<uuid>/download/<file>.csv\n",
        "    1) Warm up resource page to get cookies\n",
        "    2) Download CSV with same session + referer\n",
        "    Returns raw bytes.\n",
        "    \"\"\"\n",
        "    u = _urlparse(url)\n",
        "    path = u.path\n",
        "    # Strip trailing /download/... to get the resource page as referer\n",
        "    # e.g. /dataset/<dsid>/resource/<uuid>/download/file.csv -> /dataset/<dsid>/resource/<uuid>/\n",
        "    referer_path = re.sub(r'/download/.*$', '/', path)\n",
        "    referer = f\"{u.scheme}://{u.netloc}{referer_path}\"\n",
        "\n",
        "    with requests.Session() as s:\n",
        "        s.headers.update(_BROWSER_HEADERS)\n",
        "        # Warm up (ignore failures; some pages block bots but still set cookies)\n",
        "        try:\n",
        "            s.get(referer, timeout=30, allow_redirects=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Real download with referer\n",
        "        headers = {**_HTTP_HEADERS, \"Referer\": referer}\n",
        "        r = s.get(url, timeout=60, headers=headers, allow_redirects=True, stream=True)\n",
        "        r.raise_for_status()\n",
        "        return r.content\n",
        "\n",
        "def _read_ckan_datastore(base: str, resource_id: str, page_size: int = 50000, max_pages: int = 20) -> pd.DataFrame:\n",
        "    frames, offset = [], 0\n",
        "    api = f\"{base}/api/3/action/datastore_search\"\n",
        "    with requests.Session() as s:\n",
        "        s.headers.update({**_HTTP_HEADERS, \"X-Requested-With\": \"XMLHttpRequest\"})\n",
        "        for _ in range(max_pages):\n",
        "            r = s.get(api, params={\"resource_id\": resource_id, \"limit\": page_size, \"offset\": offset}, timeout=60)\n",
        "            r.raise_for_status()\n",
        "            res = r.json().get(\"result\", {})\n",
        "            recs = res.get(\"records\", [])\n",
        "            if not recs:\n",
        "                break\n",
        "            frames.append(pd.DataFrame.from_records(recs))\n",
        "            offset += len(recs)\n",
        "            if len(recs) < page_size:\n",
        "                break\n",
        "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "\n",
        "def _read_csv_http_with_fallback(url: str) -> pd.DataFrame:\n",
        "    # 1) Try browser-ish direct download\n",
        "    try:\n",
        "        raw = _download_browserish_csv(url)\n",
        "        delim = _sniff_delimiter(raw[:64 * 1024])\n",
        "        return pd.read_csv(io.BytesIO(raw), delimiter=delim)\n",
        "    except requests.HTTPError as e:\n",
        "        # 2) If 403, try CKAN datastore (if we have a resource UUID)\n",
        "        status = getattr(e.response, \"status_code\", None)\n",
        "        if status == 403:\n",
        "            m = re.search(r\"/resource/([0-9a-f-]{36})/\", url, flags=re.I)\n",
        "            if m:\n",
        "                base = f\"{_urlparse(url).scheme}://{_urlparse(url).netloc}\"\n",
        "                df = _read_ckan_datastore(base, m.group(1))\n",
        "                if not df.empty:\n",
        "                    return df\n",
        "        # else rethrow\n",
        "        raise\n",
        "\n",
        "def _safe_name(name: str) -> str:\n",
        "    name = name.strip().replace(\" \", \"_\")\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", name)\n",
        "\n",
        "def _sniff_delimiter(sample_bytes: bytes) -> str:\n",
        "    try:\n",
        "        sample = sample_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
        "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"|\",\"\\t\"])\n",
        "        return dialect.delimiter\n",
        "    except Exception:\n",
        "        return \",\"\n",
        "\n",
        "def _read_csv_smart(path_or_url: str) -> pd.DataFrame:\n",
        "    # Local file?\n",
        "    if not _is_http_url(path_or_url):\n",
        "        try:\n",
        "            return pd.read_csv(path_or_url)\n",
        "        except Exception:\n",
        "            with open(path_or_url, \"rb\") as f:\n",
        "                sample = f.read(64 * 1024)\n",
        "            delim = _sniff_delimiter(sample)\n",
        "            return pd.read_csv(path_or_url, delimiter=delim)\n",
        "\n",
        "    # HTTP(S)\n",
        "    try:\n",
        "        # First try a simple read (fast path); pandas may succeed for many hosts\n",
        "        return pd.read_csv(path_or_url)\n",
        "    except Exception:\n",
        "        # Use robust path with headers + CKAN fallback\n",
        "        return _read_csv_http_with_fallback(path_or_url)\n",
        "\n",
        "\n",
        "def _collect_csv_like(folder: str) -> List[str]:\n",
        "    outp = []\n",
        "    for root, _, files_ in os.walk(folder):\n",
        "        for f in files_:\n",
        "            if f.lower().endswith((\".csv\", \".data\")):\n",
        "                outp.append(os.path.join(root, f))\n",
        "    return outp\n",
        "\n",
        "def _ensure_folder(folder: str):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "def extract_uci_dataset_id(url_or_id: str):\n",
        "    s = str(url_or_id).strip()\n",
        "    if s.isdigit():\n",
        "        return int(s)\n",
        "    path = urlparse(s).path\n",
        "    m = re.search(r'/dataset/(\\d+)(?:/|$)', path)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _load_by_id(dsid: int, folder: str, preview_rows: int, save_csv: bool):\n",
        "    ds = fetch_ucirepo(id=int(dsid))\n",
        "    X, y = ds.data.features, ds.data.targets\n",
        "    df = pd.concat([X, y], axis=1) if y is not None else X\n",
        "    key = re.sub(r'[^A-Za-z0-9_]+','_', f\"uci_{ds.metadata.id}_{ds.metadata.name}\").lower()\n",
        "    globals().setdefault(\"DATASETS\", {})[key] = df\n",
        "    if save_csv:\n",
        "        _ensure_folder(folder)\n",
        "        df.to_csv(os.path.join(folder, f\"{key}.csv\"), index=False)\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(f\"‚úÖ Loaded UCI {ds.metadata.id}: {ds.metadata.name} ‚Äî shape {df.shape}\")\n",
        "        display(df.head(preview_rows))\n",
        "        print(f\"üß∞ Access with: DATASETS['{key}']\")\n",
        "\n",
        "def _load_csv_from_url(url: str, folder: str, preview_rows: int, save_csv: bool, key_prefix=\"ext\"):\n",
        "    df = _read_csv_smart(url)\n",
        "    name = os.path.basename(urlparse(url).path) or \"data.csv\"\n",
        "    name = re.sub(r'\\.csv$', '', name, flags=re.I)\n",
        "    key = re.sub(r'[^A-Za-z0-9_]+','_', f\"{key_prefix}_{name}\").lower().strip('_')\n",
        "    globals().setdefault(\"DATASETS\", {})[key] = df\n",
        "    if save_csv:\n",
        "        _ensure_folder(folder)\n",
        "        df.to_csv(os.path.join(folder, f\"{key}.csv\"), index=False)\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(f\"‚úÖ Loaded CSV from URL ‚Äî shape {df.shape}\")\n",
        "        display(df.head(preview_rows))\n",
        "        print(f\"üß∞ Access with: DATASETS['{key}']\")\n",
        "\n",
        "# -------- UCI search (unchanged) --------\n",
        "def _search_uci_datasets(query: str, limit: int = 25):\n",
        "    if not query.strip():\n",
        "        return []\n",
        "    url = f\"https://archive.ics.uci.edu/datasets?search={quote(query.strip())}\"\n",
        "    try:\n",
        "        html = requests.get(url, timeout=30, headers=_HTTP_HEADERS).text\n",
        "    except Exception:\n",
        "        return []\n",
        "    matches = re.findall(r'href=\"/dataset/(\\d+)/([^\"]+)\"', html)\n",
        "    outl, seen = [], set()\n",
        "    for id_str, slug in matches:\n",
        "        if id_str in seen:\n",
        "            continue\n",
        "        seen.add(id_str)\n",
        "        pretty = re.sub(r'[\\+\\-_]+', ' ', slug).title()\n",
        "        label = f\"{pretty} (id {id_str})\"\n",
        "        outl.append((label, int(id_str)))\n",
        "        if len(outl) >= limit:\n",
        "            break\n",
        "    return outl\n",
        "\n",
        "# -------- data.gov search --------\n",
        "def _search_datagov_csv_resources(query: str, limit: int = 50):\n",
        "    \"\"\"\n",
        "    Uses CKAN search on catalog.data.gov and returns CSV resources.\n",
        "    Output: list of tuples (label, url)\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return []\n",
        "    api = \"https://catalog.data.gov/api/3/action/package_search\"\n",
        "    try:\n",
        "        r = requests.get(api, params={\"q\": query, \"rows\": limit}, timeout=30, headers=_HTTP_HEADERS)\n",
        "        r.raise_for_status()\n",
        "        out, seen = [], set()\n",
        "        for pkg in r.json().get(\"result\", {}).get(\"results\", []):\n",
        "            pkg_title = pkg.get(\"title\") or pkg.get(\"name\") or \"dataset\"\n",
        "            for res in pkg.get(\"resources\", []):\n",
        "                fmt = (res.get(\"format\") or \"\").lower()\n",
        "                mtype = (res.get(\"mimetype\") or \"\").lower()\n",
        "                url = res.get(\"url\") or \"\"\n",
        "                if not url:\n",
        "                    continue\n",
        "                is_csv = fmt == \"csv\" or \"text/csv\" in mtype or url.lower().endswith(\".csv\")\n",
        "                if not is_csv:\n",
        "                    continue\n",
        "                label = f\"{pkg_title} ‚Äî {res.get('name') or 'CSV'}\"\n",
        "                if url in seen:\n",
        "                    continue\n",
        "                seen.add(url)\n",
        "                out.append((label, url))\n",
        "        return out\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# --- UI show/hide logic ---\n",
        "def _show(w, on=True):\n",
        "    w.layout.display = '' if on else 'none'\n",
        "\n",
        "def _update_ui(change=None):\n",
        "    mode = data_source_dropdown.value\n",
        "    _show(uci_pick_section,       mode == 'uci_search')\n",
        "    _show(datagov_pick_section,   mode == 'datagov_search')\n",
        "    _show(uci_paste_section,      mode == 'uci_paste')\n",
        "    if mode in ('uci_search', 'datagov_search'):\n",
        "        upload_button.layout.display = 'none'\n",
        "        run_button.layout.display    = 'none'   # use respective \"Load Selected\" button\n",
        "    elif mode == 'uci_paste':\n",
        "        upload_button.layout.display = 'none'\n",
        "        run_button.layout.display    = ''       # Run for pasted IDs/URLs\n",
        "    else:  # csv\n",
        "        upload_button.layout.display = ''       # upload files\n",
        "        run_button.layout.display    = ''       # Run to parse/preview\n",
        "\n",
        "data_source_dropdown.observe(_update_ui, names='value')\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_upload_clicked(_):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        if files is None:\n",
        "            print(\"Uploads not available here.\"); return\n",
        "        print(\"Select one or more CSV/ZIP files‚Ä¶\")\n",
        "        uploaded = files.upload()\n",
        "        folder = target_folder_text.value.strip() or \"datasets\"\n",
        "        _ensure_folder(folder)\n",
        "        saved = []\n",
        "        for name, data in uploaded.items():\n",
        "            path = os.path.join(folder, _safe_name(name))\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "            saved.append(path)\n",
        "        for p in saved:\n",
        "            if p.lower().endswith(\".zip\"):\n",
        "                with zipfile.ZipFile(p, \"r\") as zf:\n",
        "                    zf.extractall(folder)\n",
        "                os.remove(p)\n",
        "        print(\"‚úÖ Upload complete. Files saved to:\", os.path.abspath(folder))\n",
        "\n",
        "@contextmanager\n",
        "def busy(msg=\"Loading data‚Ä¶\", buttons=()):\n",
        "    # disable buttons to prevent double-clicks\n",
        "    for b in buttons:\n",
        "        b.disabled = True\n",
        "\n",
        "    # clear only the Output panel, then show the message\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(f\"‚è≥ {msg}\")\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        for b in buttons:\n",
        "            b.disabled = False\n",
        "\n",
        "def on_run_clicked(_):\n",
        "    # reset only the Output panel\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(\"‚è≥ Loading‚Ä¶\")\n",
        "\n",
        "    folder = target_folder_text.value.strip() or \"datasets\"\n",
        "    _ensure_folder(folder)\n",
        "\n",
        "    mode = data_source_dropdown.value\n",
        "    if mode == 'uci_paste':\n",
        "        items = [u.strip() for u in uci_urls_text.value.split(\",\") if u.strip()]\n",
        "        with out:\n",
        "            if not items:\n",
        "                print(\"‚ö†Ô∏è Please provide at least one UCI dataset ID or CSV URL.\")\n",
        "                return\n",
        "        for u in items:\n",
        "            dsid = extract_uci_dataset_id(u)\n",
        "            try:\n",
        "                if dsid is not None:\n",
        "                    _load_by_id(dsid, folder, preview_rows_int.value, save_csv_check.value)\n",
        "                else:\n",
        "                    _load_csv_from_url(u, folder, preview_rows_int.value, save_csv_check.value, key_prefix=\"url\")\n",
        "            except Exception as e:\n",
        "                with out:\n",
        "                    print(f\"‚ùå Failed to load '{u}': {e}\")\n",
        "        return\n",
        "\n",
        "    # csv mode\n",
        "    csv_like = _collect_csv_like(folder)\n",
        "    with out:\n",
        "        if not csv_like:\n",
        "            print(\"‚ö†Ô∏è No CSV/DATA files found in:\", os.path.abspath(folder))\n",
        "            return\n",
        "\n",
        "        dataframes: Dict[str, pd.DataFrame] = {}\n",
        "        print(f\"‚úÖ Discovered {len(csv_like)} CSV-like file(s):\")\n",
        "        for p in sorted(csv_like):\n",
        "            print(\" ‚Ä¢\", os.path.relpath(p, folder))\n",
        "            try:\n",
        "                df = _read_csv_smart(p)\n",
        "                key = pathlib.Path(p).stem\n",
        "                base, i = key, 2\n",
        "                while key in dataframes:\n",
        "                    key = f\"{base}_{i}\"; i += 1\n",
        "                dataframes[key] = df\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚Ü≥ ‚ùå Failed to read: {e}\")\n",
        "\n",
        "        print(\"\\nüìã Preview:\")\n",
        "        for name, df in dataframes.items():\n",
        "            print(f\"\\n{name} ‚Äî shape: {df.shape}\")\n",
        "            display(df.head(preview_rows_int.value))\n",
        "\n",
        "        globals()[\"DATASETS\"] = dataframes\n",
        "        globals()[\"DATASETS_FOLDER\"] = os.path.abspath(folder)\n",
        "        print(f\"\\nüß∞ Use DATASETS['name'] to access a DataFrame.\")\n",
        "        print(f\"üìÇ Files are under: {os.path.abspath(folder)}\")\n",
        "\n",
        "\n",
        "def _on_search_uci(_):\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(\"Searching UCI‚Ä¶\")\n",
        "    results = _search_uci_datasets(uci_search_text.value)\n",
        "    uci_results_dropdown.options = results\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(\"No matches. Try another keyword.\" if not results\n",
        "              else f\"Found {len(results)} match(es). Pick one, then click 'Load Selected'.\")\n",
        "\n",
        "def _on_load_selected_uci(_):\n",
        "    dsid = uci_results_dropdown.value\n",
        "    if dsid is None:\n",
        "        out.clear_output(wait=True)\n",
        "        with out: print(\"Pick a result first.\");\n",
        "        return\n",
        "    with busy(\"Loading UCI dataset‚Ä¶\", buttons=(btn_search_uci, btn_load_selected_uci)):\n",
        "        _load_by_id(dsid, target_folder_text.value or \"datasets\", preview_rows_int.value, save_csv_check.value)\n",
        "\n",
        "\n",
        "def _on_search_datagov(_):\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(\"Searching data.gov‚Ä¶ (CSV resources)\")\n",
        "    results = _search_datagov_csv_resources(datagov_search_text.value)\n",
        "    datagov_results_dropdown.options = results\n",
        "    out.clear_output(wait=True)\n",
        "    with out:\n",
        "        print(\"No matches or no CSV resources. Try another keyword.\" if not results\n",
        "              else f\"Found {len(results)} CSV resource(s). Pick one, then click 'Load Selected'.\")\n",
        "\n",
        "def _on_load_selected_datagov(_):\n",
        "    url = datagov_results_dropdown.value\n",
        "    if not url:\n",
        "        out.clear_output(wait=True)\n",
        "        with out: print(\"Pick a result first.\");\n",
        "        return\n",
        "    with busy(\"Loading data.gov CSV‚Ä¶\", buttons=(btn_search_datagov, btn_load_selected_datagov)):\n",
        "        _load_csv_from_url(url, target_folder_text.value or \"datasets\", preview_rows_int.value, save_csv_check.value, key_prefix=\"datagov\")\n",
        "\n",
        "\n",
        "# bind AFTER definitions\n",
        "upload_button.on_click(on_upload_clicked)\n",
        "run_button.on_click(on_run_clicked)\n",
        "btn_search_uci.on_click(_on_search_uci)\n",
        "btn_load_selected_uci.on_click(_on_load_selected_uci)\n",
        "btn_search_datagov.on_click(_on_search_datagov)\n",
        "btn_load_selected_datagov.on_click(_on_load_selected_datagov)\n",
        "\n",
        "# initial visibility + render\n",
        "def _render():\n",
        "    _update_ui()\n",
        "    display(widgets.VBox([\n",
        "        data_source_dropdown,\n",
        "        uci_pick_section,\n",
        "        datagov_pick_section,   # NEW\n",
        "        uci_paste_section,\n",
        "        settings_row,\n",
        "        action_row,\n",
        "        out\n",
        "    ]))\n",
        "\n",
        "_render()\n"
      ],
      "metadata": {
        "id": "RgBibw6HffL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 4 ‚Äî Provide a summary of the data from your previous selection** (require Gemini API) { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click **‚ñ∂Ô∏è Run**.\n",
        "#@markdown <br>**What this does:** Generate a summary of the data using Gemini API.\n",
        "\n",
        "PROMPT_BLOB = \"eJyVk8GO1DAMhl_Fx5mqrZZdlsNcEBKCAwIhBrS3RW7jacOkcRUn0-2eeAfekCfBactKnIBbEv-xf39O3jIb-MYNGGrZWN9B7K3AQCLYEZjd_df7fXOoquoTOzrAkbzlAAYjAnp0s8QaXiVjybca9uyrSG3vbYuufsPO8aQZSQhCciRAD9hGNx8qOEYMEWZOAQKNbobJxj5rVw046zVhUTzbw12PEdTVYm0t3XCKL4uigs89ebC-dcmQqq839dl6A3zSyInDgNGyB8MwEfR4Ib2p7o3qbzZ9iz4Hs2S1sVVaanxMEQjbHoTaJVNPuLDSpY0CPPnFbqnloGFnyt99AgpIr_G6gi8K4T2Gs8ly9m6u4QPrjRgYIj3EUumBDoGgcdyeZdlHbBTbsuw52Ef2Ed3Kcjk0VlqHdqAgWuId0aiOIHlDAW6vrmDiYHLkTjHRRQ8VuE7XL22gn7WgS4OX7Dyzzz0LRdAZr2OLFAY5qI_QkVocrEju_IIuZQs6Bme1eAlqQzTN0KBDfQsAU8AxJxly7qLIXBT7juquLnW_ZiyK_XGDaiiidXL468ivf37_8Rya5JwaHVkJKuQ0DBjm-j9eQAWvc944j7mRMz2xKLduYWdPmdG-fMIi9pFquFEDL_40UP_jU8pXb2EM-jryF9n-kH4MBX6xktBphaDtdB3J4jmyimSiUP8CmohFxw==\"\n",
        "\n",
        "def get_caesar_command(msg):\n",
        "    s = msg.strip()\n",
        "    s += \"=\" * (-len(s) % 4)                 # fix missing padding\n",
        "    raw = base64.urlsafe_b64decode(s)        # URL-safe decode\n",
        "    return zlib.decompress(raw).decode(\"utf-8\")\n",
        "\n",
        "def keep_after_marker(text: str, marker: str = \"---\") -> str:\n",
        "    i = text.lower().find(marker.lower())\n",
        "    if i != -1:\n",
        "        return text[i:].lstrip()\n",
        "    parts = re.split(r'(?m)^\\s*---+\\s*$', text, maxsplit=1)\n",
        "    return parts[1].lstrip() if len(parts) > 1 else text\n",
        "\n",
        "def build_llm_dataset_report(datasets: dict,\n",
        "                             max_rows: int = 5,\n",
        "                             max_cols_listed: int = 40,\n",
        "                             max_chars: int = 60000) -> str:\n",
        "\n",
        "    if not isinstance(datasets, dict) or not datasets:\n",
        "        return \"No datasets loaded.\"\n",
        "\n",
        "    parts = []\n",
        "    for name, df in datasets.items():\n",
        "        # info()\n",
        "        info_buf = io.StringIO()\n",
        "        try:\n",
        "            df.info(buf=info_buf)\n",
        "            info_txt = info_buf.getvalue()\n",
        "        except Exception:\n",
        "            info_txt = \"(info unavailable)\"\n",
        "\n",
        "        # describe() with fallback\n",
        "        try:\n",
        "            desc = df.describe(include=\"all\").T\n",
        "        except Exception:\n",
        "            desc = df.select_dtypes(include=\"number\").describe().T\n",
        "\n",
        "        # robust datetime summary\n",
        "        dt_cols = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]\n",
        "        dt_txt = \"\"\n",
        "        if dt_cols:\n",
        "            dt_num = pd.DataFrame({\n",
        "                c: pd.to_datetime(df[c], utc=True, errors=\"coerce\").astype(\"int64\") / 1e9\n",
        "                for c in dt_cols\n",
        "            })\n",
        "            dt_txt = \"\\n-- datetime (secs since epoch) describe --\\n\" + dt_num.describe().T.to_string()\n",
        "\n",
        "        # limit columns listing\n",
        "        columns_list = list(map(str, df.columns[:max_cols_listed]))\n",
        "        if df.shape[1] > max_cols_listed:\n",
        "            columns_list.append(\"...\")\n",
        "\n",
        "        head_txt = df.head(max_rows).to_string(index=False)\n",
        "\n",
        "        part = (\n",
        "            f\"\\n### DATASET: {name}\\n\"\n",
        "            f\"shape: {df.shape}\\n\"\n",
        "            f\"columns: {', '.join(columns_list)}\\n\"\n",
        "            f\"\\n-- head({max_rows}) --\\n{head_txt}\\n\"\n",
        "            f\"\\n-- describe --\\n{desc.to_string()}\\n\"\n",
        "            f\"{dt_txt}\\n\"\n",
        "            f\"\\n-- info --\\n{info_txt}\\n\"\n",
        "        )\n",
        "        parts.append(part)\n",
        "\n",
        "    report = \"\\n\" + (\"\\n\" + \"=\"*70 + \"\\n\").join(parts)\n",
        "    if len(report) > max_chars:\n",
        "        report = report[:max_chars] + \"\\n...[truncated]...\"\n",
        "    return report\n",
        "\n",
        "# Get Gemini report\n",
        "if 'DATASETS' in globals() and DATASETS:\n",
        "    if '_gemini_model' in globals() and _gemini_model is not None:\n",
        "        report_text = build_llm_dataset_report(DATASETS)\n",
        "        prompt = keep_after_marker(get_caesar_command(PROMPT_BLOB))\n",
        "        reply = gemini_chat(prompt+report_text)\n",
        "        display(HTML(\"\"\"\n",
        "          <style>\n",
        "            .output .markdown, .output .markdown * {\n",
        "              font-size: 24px !important;\n",
        "              line-height: 1.6 !important;\n",
        "            }\n",
        "          </style>\n",
        "        \"\"\"))\n",
        "        display(Markdown(reply))\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Gemini API not initialized. Please run Step 2 to enable Gemini features.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No datasets loaded. Please run Step 3 to load a dataset.\")"
      ],
      "metadata": {
        "id": "pkujiSE1Wc8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 5 ‚Äî Provide a Plots & Charts using LLM** (require Gemini API) { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click **‚ñ∂Ô∏è Run**.\n",
        "#@markdown <br>**What this does:** Plots & Charts using LLM.\n",
        "\n",
        "PROMPT_BLOB = \"eJydUstuE0EQvO9XlOaUSHaCxENihXKAQwRCNnJOESJW727bHjw7beaBvUSR8g_whfkSetcOIhFw4DbTU13VNV3nIg0-S4WGa2msXyKtbETLMdKS0Rxdza-Oq7IYj8fFjFMOHtPJ-0u8u5hOcOQFLYV1I1s_gl42QSIfYyHOyfYXWaxX3FJZXBeAqVcUUjTlR70Ap6d4I9knyAKEmhIvJdiaHGpxufUD6NqkbsOmNHUPNSOz0_Or38Bn5mZ0T3ceJG-4QUUBAz4-5ND6fLnH_JFpZFaZ_8F_kaheK7-X0JKz3_SorjcSkhUfUXUY2B-Kxn3TvEf-l-pr2WHjJJXwuWV9xtd4_13dI4Oy-4tE19cO_UqubZ-Km6KYZcdRN4ypdx0CLziwr_mwgag7pATe2ZhOFPRhAGhRAY5itAul770jUVhyAkUMZrBdsUfMNlHluG99enf7_QX2AdDc7Pra237vgb9kjkm_cq8JDY0ETaPmoMr6c4p3ds0wT8bP1Mjz8UszQkzWOeTIsMNkk6m2ty17HaS7u_0xRFTU00lRXEoGBVatyN5KQEOJQCrQqS3MmJrBUl9eBGr5EFqFHParMvvJy-InkkoNYw==\"\n",
        "\n",
        "# ----------------- Utilities -----------------\n",
        "def _latest_dataset(dsets: dict) -> tuple[str, pd.DataFrame]:\n",
        "    if not isinstance(dsets, dict) or not dsets:\n",
        "        raise RuntimeError(\"No DATASETS loaded.\")\n",
        "    key = next(reversed(dsets))\n",
        "    return key, dsets[key].copy()\n",
        "\n",
        "def _infer_col_types(df: pd.DataFrame) -> dict:\n",
        "    out = {}\n",
        "    for c in df.columns:\n",
        "        dt = str(df[c].dtype)\n",
        "        if pd.api.types.is_numeric_dtype(df[c]):\n",
        "            out[c] = \"numeric\"\n",
        "        elif pd.api.types.is_bool_dtype(df[c]):\n",
        "            out[c] = \"boolean\"\n",
        "        elif pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            out[c] = \"datetime\"\n",
        "        else:\n",
        "            # treat few-unique objects as categorical\n",
        "            nunique = df[c].nunique(dropna=True)\n",
        "            out[c] = \"categorical\" if nunique <= max(30, int(len(df)*0.2)) else \"text\"\n",
        "    return out\n",
        "\n",
        "def _example_values(df, col, k=6):\n",
        "    vals = df[col].dropna().astype(str).unique()[:k]\n",
        "    return list(map(str, vals))\n",
        "\n",
        "def _build_schema_for_llm(df: pd.DataFrame, limit_rows=6):\n",
        "    # small preview + column inventory to guide Gemini\n",
        "    col_types = _infer_col_types(df)\n",
        "    cols = []\n",
        "    for c in df.columns:\n",
        "        cols.append({\n",
        "            \"name\": c,\n",
        "            \"inferred_type\": col_types[c],\n",
        "            \"examples\": _example_values(df, c)\n",
        "        })\n",
        "    sample = df.head(limit_rows).to_dict(orient=\"records\")\n",
        "    return {\"columns\": cols, \"sample_rows\": sample}\n",
        "\n",
        "def _extract_json_block(text: str):\n",
        "    # try to pull the first {...} block\n",
        "    m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "    return m.group(0) if m else None\n",
        "\n",
        "# ------------- Chart rendering helpers -------------\n",
        "def _try_order_ordinal(series: pd.Series):\n",
        "    \"\"\"\n",
        "    Attempt to order categories like '0-4','5-9', '10-14', or single numbers as strings.\n",
        "    Returns (ordered_categories, converter_func_or_None).\n",
        "    \"\"\"\n",
        "    vals = series.dropna().astype(str).unique().tolist()\n",
        "    # detect ranges \"a-b\"\n",
        "    bounds = []\n",
        "    for v in vals:\n",
        "        m = re.match(r'^\\s*(\\d+)\\s*[-‚Äì]\\s*(\\d+)\\s*$', v)\n",
        "        if m:\n",
        "            lo, hi = int(m.group(1)), int(m.group(2))\n",
        "            bounds.append((v, (lo+hi)/2))\n",
        "        else:\n",
        "            m2 = re.match(r'^\\s*(\\d+)\\s*$', v)\n",
        "            if m2:\n",
        "                bounds.append((v, float(m2.group(1))))\n",
        "            else:\n",
        "                return None, None  # give up\n",
        "    bounds.sort(key=lambda x: x[1])\n",
        "    ordered = [b[0] for b in bounds]\n",
        "    return ordered, dict(bounds)  # map label->numeric midpoint\n",
        "\n",
        "def _count_plot(df, x):\n",
        "    vc = df[x].dropna().astype(str).value_counts()\n",
        "    if vc.empty:\n",
        "        print(f\"(skip) No data for count plot of {x}\"); return\n",
        "    ax = vc.plot(kind='bar')\n",
        "    ax.set_title(f\"Count of {x}\")\n",
        "    ax.set_xlabel(x); ax.set_ylabel(\"count\")\n",
        "    plt.show()\n",
        "\n",
        "def _bar_grouped_counts(df, x, hue, stacked=False, normalize=False):\n",
        "    if x not in df or hue not in df:\n",
        "        print(f\"(skip) Missing columns for grouped bar: {x}, {hue}\"); return\n",
        "    tbl = pd.crosstab(df[x].astype(str), df[hue].astype(str), normalize='index' if normalize else False)\n",
        "    ax = tbl.plot(kind='bar', stacked=stacked)\n",
        "    ax.set_title(f\"{'Proportion' if normalize else 'Counts'} of {x} by {hue}\")\n",
        "    ax.set_xlabel(x); ax.set_ylabel(\"proportion\" if normalize else \"count\")\n",
        "    plt.legend(title=hue, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _box_by_category(df, x, y):\n",
        "    if x not in df or y not in df:\n",
        "        print(f\"(skip) Missing columns for boxplot: {x}, {y}\"); return\n",
        "    s_x = df[x].astype(str)\n",
        "    y_num = pd.to_numeric(df[y], errors='coerce')\n",
        "    if y_num.notna().sum() < 2:\n",
        "        print(f\"(skip) Not enough numeric data for {y}\"); return\n",
        "\n",
        "    # try ordinal ordering on x\n",
        "    ordered, mapping = _try_order_ordinal(s_x)\n",
        "    if ordered:\n",
        "        s_x = pd.Categorical(s_x, categories=ordered, ordered=True)\n",
        "    groups = [y_num[s_x == lvl].dropna().values for lvl in pd.Series(s_x).cat.categories] if hasattr(s_x, \"cat\") else \\\n",
        "             [y_num[s_x == lvl].dropna().values for lvl in sorted(s_x.unique())]\n",
        "    labels = (list(pd.Series(s_x).cat.categories) if hasattr(s_x, \"cat\") else sorted(s_x.unique()))\n",
        "    if len(labels) < 2:\n",
        "        print(f\"(skip) Only one category in {x}\"); return\n",
        "\n",
        "    plt.figure()\n",
        "    plt.boxplot(groups, labels=labels, showfliers=False)\n",
        "    plt.title(f\"{y} by {x}\")\n",
        "    plt.xlabel(x); plt.ylabel(y)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------- End-to-end: ask Gemini -> render -------------\n",
        "def request_chart_plan_from_gemini(df: pd.DataFrame, dataset_observation: str) -> dict:\n",
        "    \"\"\"\n",
        "    Ask Gemini for a JSON chart plan. Expects your `gemini_chat()` wrapper to be available.\n",
        "    \"\"\"\n",
        "    schema = _build_schema_for_llm(df)\n",
        "    prompt = (keep_after_marker(get_caesar_command(PROMPT_BLOB))+\n",
        "              f\"SCHEMA:\\n{json.dumps(schema, ensure_ascii=False)}\\n\\n\"\n",
        "              f\"Dataset observation: \\n{dataset_observation}\"\n",
        "             )\n",
        "    raw = gemini_chat(prompt)  # uses your existing wrapper\n",
        "    blob = _extract_json_block(raw) or raw\n",
        "    try:\n",
        "        plan = json.loads(blob)\n",
        "        assert isinstance(plan, dict) and \"charts\" in plan\n",
        "        return plan\n",
        "    except Exception as e:\n",
        "        print(\"Could not parse Gemini response as JSON. Raw response:\\n\", raw)\n",
        "        raise\n",
        "\n",
        "def render_chart_plan(df: pd.DataFrame, plan: dict):\n",
        "    for spec in plan.get(\"charts\", []):\n",
        "        t = spec.get(\"type\")\n",
        "        try:\n",
        "            if t == \"count\":\n",
        "                _count_plot(df, spec[\"x\"])\n",
        "            elif t == \"bar_grouped\":\n",
        "                _bar_grouped_counts(df, spec[\"x\"], spec[\"hue\"], stacked=False, normalize=False)\n",
        "            elif t == \"stacked_prop\":\n",
        "                _bar_grouped_counts(df, spec[\"x\"], spec[\"hue\"], stacked=True, normalize=True)\n",
        "            elif t == \"box\":\n",
        "                _box_by_category(df, spec[\"x\"], spec[\"y\"])\n",
        "            else:\n",
        "                print(f\"(skip) Unknown chart type: {t}\")\n",
        "        except Exception as e:\n",
        "            print(f\"(skip) Failed to render {t}: {e}\")\n",
        "\n",
        "# Use dataset summary from step 4\n",
        "if 'DATASETS' in globals() and DATASETS:\n",
        "    if '_gemini_model' in globals() and _gemini_model is not None:\n",
        "        key, df0 = _latest_dataset(DATASETS)\n",
        "\n",
        "        # Get the dataset observation from the previous Gemini reply stored in globals\n",
        "        # Check if `reply` is defined and not empty\n",
        "        dataset_observation = globals().get('reply', '') # Get the 'reply' variable, default to empty string if not found\n",
        "\n",
        "        if not dataset_observation:\n",
        "             print(\"‚ö†Ô∏è Dataset observation from Step 4 is not available or is empty. Please run Step 4 first.\")\n",
        "        else:\n",
        "            # 1) Ask Gemini for a plan\n",
        "            plan = request_chart_plan_from_gemini(df0, dataset_observation)\n",
        "            print(\"Based on the dataset provided, below are a few plots that can help visualize your data: \", plan)  # so you can see what it decided\n",
        "\n",
        "            # 2) Render it\n",
        "            render_chart_plan(df0, plan)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Gemini API not initialized. Please run Step 2 to enable Gemini features.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No datasets loaded. Please run Step 3 to load a dataset.\")"
      ],
      "metadata": {
        "id": "FAn9LB33fdyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è **Step 6 ‚Äî Gather statistic from the dataset tables** { display-mode: \"form\" }\n",
        "#@markdown **What to do:** Click **‚ñ∂Ô∏è Run**.\n",
        "#@markdown <br>**What this does:** (use üìä button to the right of the dataframe for plots with SeaBorn)\n",
        "\n",
        "# Display the dataset and provide option for statistical analysis\n",
        "print(\"--- Loaded Datasets Summary ---\")\n",
        "if 'DATASETS' in globals() and DATASETS:\n",
        "    for name, df in DATASETS.items():\n",
        "        print(f\"Dataset: '{name}'\")\n",
        "        print(f\"  Number of records: {df.shape[0]}\")\n",
        "        print(f\"  Number of columns: {df.shape[1]}\")\n",
        "        display(df)\n",
        "else:\n",
        "    print(\"No datasets have been loaded yet.\")"
      ],
      "metadata": {
        "id": "qKfkminDKa0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}